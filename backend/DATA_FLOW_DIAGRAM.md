# ğŸ“Š DATA FLOW DIAGRAM

## ğŸ¤ AUDIO ANALYSIS FLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER UPLOADS AUDIO FILE                          â”‚
â”‚                      (voice_recording.wav, 2MB)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    POST /analyze/audio/enhanced                   â”‚
        â”‚    Body: FormData                                 â”‚
        â”‚    file: <binary audio data>                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       STAGE 1: PRE-TRAINED MODEL (Wav2Vec2)       â•‘
        â•‘                                                   â•‘
        â•‘  Input:  audio_bytes (raw WAV data)              â•‘
        â•‘  Model:  ehcalabres/wav2vec2-lg-xlsr...          â•‘
        â•‘  Process: Extract acoustic features               â•‘
        â•‘          Classify emotions from voice patterns    â•‘
        â•‘                                                   â•‘
        â•‘  Output: {                                        â•‘
        â•‘    "score": 0.68,                                â•‘
        â•‘    "bucket": "Moderate",                         â•‘
        â•‘    "explain": {                                  â•‘
        â•‘      "dominant_emotion": "stress",               â•‘
        â•‘      "emotion_distribution": {                   â•‘
        â•‘        "stress": 0.45,                          â•‘
        â•‘        "sadness": 0.25,                         â•‘
        â•‘        "neutral": 0.20,                         â•‘
        â•‘        "happy": 0.10                            â•‘
        â•‘      }                                           â•‘
        â•‘    }                                             â•‘
        â•‘  }                                               â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            â”‚
                            â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       STAGE 2: LLM ENHANCEMENT (Llama 3.1 8B)    â•‘
        â•‘                                                   â•‘
        â•‘  Input:  Model output from Stage 1                â•‘
        â•‘  LLM:    Groq API (llama-3.1-8b-instant)         â•‘
        â•‘  Process: Contextual interpretation               â•‘
        â•‘          Mental health insights                   â•‘
        â•‘          Actionable recommendations               â•‘
        â•‘                                                   â•‘
        â•‘  Prompt: "Analyze this voice emotion data..."     â•‘
        â•‘                                                   â•‘
        â•‘  Output: {                                        â•‘
        â•‘    "enhanced": true,                             â•‘
        â•‘    "interpretation": "High stress detected,       â•‘
        â•‘                       may indicate anxiety...",   â•‘
        â•‘    "concern_level": "Moderate",                  â•‘
        â•‘    "concern_reason": "Persistent stress with     â•‘
        â•‘                       sadness suggests...",       â•‘
        â•‘    "actionable_tip": "Practice deep breathing    â•‘
        â•‘                       exercises for 10 mins..."   â•‘
        â•‘  }                                               â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         RETURN TO FRONTEND (JSON Response)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ IMAGE ANALYSIS FLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER UPLOADS IMAGE FILE                          â”‚
â”‚                         (selfie.jpg, 1.5MB)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    POST /analyze/image/enhanced                   â”‚
        â”‚    Body: FormData                                 â”‚
        â”‚    file: <binary image data>                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       STAGE 1: PRE-TRAINED MODEL (FER + MTCNN)   â•‘
        â•‘                                                   â•‘
        â•‘  Input:  image_bytes (JPEG/PNG data)             â•‘
        â•‘  Models: MTCNN (face detection)                  â•‘
        â•‘          FER trained on FER2013 dataset          â•‘
        â•‘  Process: Detect face in image                    â•‘
        â•‘          Extract facial landmarks                 â•‘
        â•‘          Classify expressions                     â•‘
        â•‘                                                   â•‘
        â•‘  Output: {                                        â•‘
        â•‘    "score": 0.42,                                â•‘
        â•‘    "bucket": "Low",                              â•‘
        â•‘    "explain": {                                  â•‘
        â•‘      "face_detected": true,                      â•‘
        â•‘      "dominant_emotion": "happy",                â•‘
        â•‘      "confidence": 0.82                          â•‘
        â•‘    },                                            â•‘
        â•‘    "top_emotions": [                             â•‘
        â•‘      {"emotion": "happy", "score": 0.65},       â•‘
        â•‘      {"emotion": "neutral", "score": 0.25},     â•‘
        â•‘      {"emotion": "surprise", "score": 0.10}     â•‘
        â•‘    ]                                             â•‘
        â•‘  }                                               â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            â”‚
                            â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       STAGE 2: LLM ENHANCEMENT (Llama 3.1 8B)    â•‘
        â•‘                                                   â•‘
        â•‘  Input:  Model output from Stage 1                â•‘
        â•‘  LLM:    Groq API (llama-3.1-8b-instant)         â•‘
        â•‘  Process: Mood interpretation                     â•‘
        â•‘          Pattern recognition                      â•‘
        â•‘          Personalized tips                        â•‘
        â•‘                                                   â•‘
        â•‘  Prompt: "Analyze this facial expression..."      â•‘
        â•‘                                                   â•‘
        â•‘  Output: {                                        â•‘
        â•‘    "enhanced": true,                             â•‘
        â•‘    "mood_interpretation": "Predominantly         â•‘
        â•‘                            positive mood with     â•‘
        â•‘                            content feeling...",   â•‘
        â•‘    "patterns_to_monitor": "Watch for sudden      â•‘
        â•‘                            mood shifts or...",    â•‘
        â•‘    "mood_boost_tip": "Practice gratitude by      â•‘
        â•‘                       writing 3 things you're     â•‘
        â•‘                       thankful for daily..."      â•‘
        â•‘  }                                               â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         RETURN TO FRONTEND (JSON Response)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª WHAT I TESTED

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          MY TEST (test_audio_image_llm.py)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SKIPPED: Real file upload              â”‚
        â”‚  SKIPPED: Pre-trained model processing  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘  SIMULATED: Model output (Stage 1)      â•‘
        â•‘                                         â•‘
        â•‘  audio_result = {                       â•‘
        â•‘    "score": 0.68,                      â•‘
        â•‘    "explain": {                        â•‘
        â•‘      "dominant_emotion": "stress",     â•‘
        â•‘      ...                               â•‘
        â•‘    }                                   â•‘
        â•‘  }                                     â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          â”‚
                          â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘  TESTED: LLM Enhancement (Stage 2)      â•‘
        â•‘                                         â•‘
        â•‘  enhanced = enhance_audio_analysis(     â•‘
        â•‘    audio_result                        â•‘
        â•‘  )                                     â•‘
        â•‘                                         â•‘
        â•‘  âœ… Verified LLM reasoning works        â•‘
        â•‘  âœ… Verified JSON parsing works         â•‘
        â•‘  âœ… Verified Groq API works             â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ’¡ KEY INSIGHT

**I tested ONLY the LLM enhancement part** because:

1. âœ… Pre-trained models (Wav2Vec2, FER) are already proven to work
2. âœ… We need to verify the NEW code (LLM integration)
3. âœ… Simulating model output is faster than processing real files
4. âœ… Focuses testing on the logic we just added

**To test the FULL pipeline with real files:**
- Start the server
- Use the API docs at http://localhost:8000/docs
- Upload a real .wav audio file or .jpg image
- See both Stage 1 (model) AND Stage 2 (LLM) work together!

---

## ğŸ“ FILE STRUCTURE

```
backend/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ audio_infer.py          â† Stage 1 (Wav2Vec2 model)
â”‚   â”œâ”€â”€ image_infer.py          â† Stage 1 (FER model)
â”‚   â””â”€â”€ llm_enhance.py          â† Stage 2 (LLM reasoning) âœ¨ NEW
â”‚
â”œâ”€â”€ app.py                      â† API endpoints
â”‚
â””â”€â”€ test_audio_image_llm.py     â† Test script (simulates Stage 1)
```
