# MindEase - Quick Start Guide

## ğŸš€ Getting Started (For Judges)

### **What is MindEase?**
An AI-powered mental health monitoring system that analyzes your text, voice, and facial expressions to detect early signs of stress and depression.

---

## ğŸ“‹ Prerequisites

- Python 3.10+
- Node.js 18+
- 8GB RAM
- 5GB disk space

---

## âš¡ Quick Setup (5 minutes)

### **Step 1: Clone Repository**
```bash
git clone https://github.com/aaroohhiiii/mentalHealth.git
cd mentalHealth
```

### **Step 2: Start Backend**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python -m uvicorn app:app --host 127.0.0.1 --port 8000
```

**Note:** First run downloads ML models (~2GB). Wait for "Application startup complete".

### **Step 3: Start Frontend** (New Terminal)
```bash
cd frontend
npm install
npm run dev
```

### **Step 4: Open Application**
Visit: **http://localhost:5173**

---

## ğŸ¯ How to Use

### **1. Home Page**
- See system overview
- Click "New Entry" to start

### **2. Daily Check-in**

#### **ğŸ“ Text Analysis**
```
Type something like:
"I feel overwhelmed with work and can't sleep well. 
Everything feels too much lately."
```
Click "Analyze Text" â†’ See results instantly

#### **ğŸ¤ Audio Analysis**
1. Click "ğŸ¤ Start Recording"
2. Speak for 5-10 seconds about how you feel
3. Click "â¹ï¸ Stop Recording"
4. Click "ğŸ” Analyze Audio"

#### **ğŸ“¸ Selfie Analysis**
1. Click "ğŸ“· Take Selfie" or upload image
2. Click "Analyze Image"

### **3. View Results**
- **Risk Score:** 0-100% (Low/Moderate/High)
- **Explanation:** Why this score was given
- **Suggestions:** Personalized recommendations

### **4. Track Trends**
- Click "Trends" tab
- See 7-day visualization
- Identify patterns

---

## ğŸ¬ Demo Flow (For Presentation)

### **Scenario: Student with Exam Stress**

**1. Text Input:**
```
"I have finals coming up and I'm really anxious. 
Can't focus on studying and feeling hopeless about passing."
```
**Expected:** Moderate-High risk (65-75%)

**2. Audio Recording:**
Speak in a stressed, quiet voice:
```
"I'm just... I don't know if I can do this anymore. 
Everything is too much."
```
**Expected:** High risk (70-85%)

**3. Selfie:**
Take photo with concerned/sad expression
**Expected:** Moderate risk (50-70%)

**4. View Results:**
- See individual modality scores
- Check fusion result
- Read personalized suggestions

**5. Navigate to Trends:**
- View historical data
- Show pattern recognition

---

## ğŸ”§ API Endpoints (For Testing)

### **Health Check**
```bash
curl http://localhost:8000/
```

### **Text Analysis**
```bash
curl -X POST "http://localhost:8000/analyze/text" \
  -H "Content-Type: application/json" \
  -d '{"text": "I feel sad and overwhelmed"}'
```

### **Audio Analysis**
```bash
curl -X POST "http://localhost:8000/analyze/audio" \
  -F "file=@recording.wav"
```

### **Image Analysis**
```bash
curl -X POST "http://localhost:8000/analyze/image" \
  -F "file=@selfie.jpg"
```

### **7-Day Trends**
```bash
curl http://localhost:8000/trend/7d
```

---

## ğŸ¨ Features to Highlight

### **1. Multi-Modal AI**
- Analyzes 3 different input types
- Combines results for holistic view

### **2. Explainable AI**
- Shows which words/patterns triggered the score
- Natural language explanations

### **3. Privacy-First**
- No data storage
- No user tracking
- All processing local

### **4. Real-Time Processing**
- Instant results (<2 seconds)
- No waiting for analysis

### **5. Trend Visualization**
- 7-day pattern tracking
- Color-coded risk levels

---

## ğŸ“ Key Technical Points

### **Models Used:**
1. **RoBERTa** - Text sentiment (125M parameters)
2. **Wav2Vec2** - Audio emotion (1.27GB model)
3. **FER CNN** - Facial expression recognition
4. **Groq Llama 3.1 8B** - LLM enhancement

### **Architecture:**
```
Frontend (React + TypeScript)
    â†“
Backend (FastAPI + Python)
    â†“
ML Models (PyTorch + Transformers)
    â†“
LLM Enhancement (Groq API)
```

### **Data Flow:**
```
User Input â†’ ML Analysis â†’ Results Display
             â†“
        (No Storage)
```

---

## ğŸ› Troubleshooting

### **Backend won't start:**
```bash
# Kill existing process
lsof -ti:8000 | xargs kill -9

# Restart
cd backend
source venv/bin/activate
python -m uvicorn app:app --host 127.0.0.1 --port 8000
```

### **Frontend not loading:**
```bash
# Check if port 5173 is busy
lsof -ti:5173

# Clear node_modules and reinstall
rm -rf node_modules
npm install
npm run dev
```

### **Models not loading:**
- First run takes time (downloading 2GB+ models)
- Check internet connection
- Wait for "âœ“ Model loaded successfully" messages

### **Audio recording not working:**
- Allow microphone permission in browser
- Use Chrome/Firefox (Safari may have issues)
- Check browser console for errors

---

## ğŸ“Š Expected Results

### **Text: "I feel great and excited!"**
- Score: 10-20% (Low risk)
- Sentiment: Positive
- Bucket: Low

### **Text: "I'm overwhelmed and can't sleep"**
- Score: 65-75% (Moderate-High risk)
- Sentiment: Negative
- Bucket: Moderate/High

### **Audio: Happy/Upbeat voice**
- Score: 15-30% (Low risk)
- Emotion: Happy/Neutral

### **Audio: Sad/Quiet voice**
- Score: 60-80% (Moderate-High risk)
- Emotion: Sad/Fear

### **Image: Smiling face**
- Score: 10-25% (Low risk)
- Expression: Happy

### **Image: Sad/Frowning face**
- Score: 55-75% (Moderate-High risk)
- Expression: Sad/Neutral

---

## ğŸ¯ Demo Tips

1. **Show the contrast:** Test with both positive and negative inputs
2. **Explain the scores:** Walk through why each score was given
3. **Highlight privacy:** Show no data is stored
4. **Demonstrate trends:** Enter multiple days of data
5. **Show explanations:** Point out the AI reasoning

---

## ğŸ“ Presentation Flow

**5-Minute Version:**
1. Introduction (30s) - What is MindEase?
2. Demo Text Analysis (1m)
3. Demo Audio Analysis (1m)
4. Demo Image Analysis (1m)
5. Show Results & Trends (1m)
6. Explain Technology (30s)

**10-Minute Version:**
1. Introduction (1m)
2. Problem Statement (1m)
3. Demo All Modalities (3m)
4. Results & Explanations (2m)
5. Technical Architecture (2m)
6. Privacy & Ethics (1m)

---

## ğŸ”— Important Links

- **GitHub:** https://github.com/aaroohhiiii/mentalHealth
- **Frontend:** http://localhost:5173
- **Backend API:** http://localhost:8000
- **API Docs:** http://localhost:8000/docs

---

## ğŸ“ Support

If you encounter issues:
1. Check `backend/server.log` for backend errors
2. Check browser console (F12) for frontend errors
3. Verify both servers are running
4. Ensure ports 8000 and 5173 are available

---

## âœ¨ Key Selling Points

âœ… **Multi-Modal Analysis** - First mental health AI to combine text+audio+image  
âœ… **Privacy-Preserving** - No data storage or tracking  
âœ… **Explainable AI** - Shows reasoning behind scores  
âœ… **Real-Time** - Instant results in under 2 seconds  
âœ… **Open Source Models** - Uses research-backed, public models  
âœ… **LLM-Enhanced** - Combines ML precision with AI reasoning  
âœ… **User-Friendly** - Simple interface, no training needed  
âœ… **Educational** - Raises mental health awareness  

---

**Ready to demo! ğŸš€**

For detailed technical documentation, see `JUDGES_DOCUMENTATION.md`
